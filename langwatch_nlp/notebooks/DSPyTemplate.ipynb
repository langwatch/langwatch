{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langwatch\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from enum import Enum\n",
      "\n",
      "\n",
      "class ModelFoo(Enum):\n",
      "    high = 'high'\n",
      "    medium = 'medium'\n",
      "    low = 'low'\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Model_Foo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m namespace \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     39\u001b[0m exec(output, namespace, namespace)\n\u001b[0;32m---> 40\u001b[0m Model \u001b[38;5;241m=\u001b[39m \u001b[43mnamespace\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     42\u001b[0m Model\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Model_Foo'"
     ]
    }
   ],
   "source": [
    "from datamodel_code_generator import InputFileType, generate\n",
    "import json\n",
    "import re\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "json_schema = {\n",
    "    \"type\": \"enum\",\n",
    "    \"enum\": [\"high\", \"medium\", \"low\"],\n",
    "    # \"properties\": {\n",
    "    #     \"importance\": {\"type\": \"number\", \"title\": \"Importance\"},\n",
    "    #     \"reasoning\": {\"type\": \"string\", \"title\": \"Reasoning\"},\n",
    "    # },\n",
    "    # # \"required\": [\"reasoning\", \"importance\"],\n",
    "    # \"title\": \"NestedAnswerImportance\",\n",
    "}\n",
    "\n",
    "# Generate code. The result is a string of the Python code.\n",
    "code_buffer = io.StringIO()\n",
    "\n",
    "model_name = json_schema.get(\"title\", \"Model\") # <- field name\n",
    "\n",
    "# Redirect stdout to the buffer while calling generate()\n",
    "with redirect_stdout(code_buffer):\n",
    "    generate(\n",
    "        json.dumps(json_schema),\n",
    "        input_file_type=InputFileType.JsonSchema,\n",
    "        class_name=model_name\n",
    "    )\n",
    "\n",
    "# Get the generated code as a string\n",
    "output = code_buffer.getvalue()\n",
    "output = re.sub(r\"# generated by[\\s\\S]*?from __future__ import annotations\", \"\", output).strip()\n",
    "output = re.sub(r\"class (.*)?\\(BaseModel\\):\\n    __root__: \", r\"\\1 = \", output).strip()\n",
    "\n",
    "print(output)\n",
    "\n",
    "namespace = {}\n",
    "exec(output, namespace, namespace)\n",
    "Model = namespace[model_name]\n",
    "\n",
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Type, cast\n",
    "import dspy\n",
    "from dspy.signatures.signature import Signature\n",
    "from dspy.adapters.types.image import try_expand_image_tags\n",
    "\n",
    "from langwatch_nlp.studio.utils import SerializableWithStringFallback\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class TemplateAdapter(dspy.JSONAdapter):\n",
    "    \"\"\"\n",
    "    This is a \"TemplateAdapter\" DSPy Adapter, that avoid modifying the messages as much as possible,\n",
    "    and instead uses a {{mustache}} template formating to fill in the inputs on the messages.\n",
    "\n",
    "    This adapter does not append any text to the system prompt like DSPy normally does and uses json for the outputs\n",
    "    by default, this matches much better what users expect comming from OpenAI standards, and will allow them to simply\n",
    "    pick up the same prompts and json schemas and use in any other frameworks as is, since all of them adhere to the\n",
    "    raw OpenAI way of interating with LLMs.\n",
    "    \"\"\"\n",
    "\n",
    "    def format(\n",
    "        self,\n",
    "        signature: Type[Signature],\n",
    "        demos: list[dict[str, Any]],\n",
    "        inputs: dict[str, Any],\n",
    "    ) -> list[dict[str, Any]]:\n",
    "        inputs_copy = dict(inputs)\n",
    "\n",
    "        # If the signature and inputs have conversation history, we need to format the conversation history and\n",
    "        # remove the history field from the signature.\n",
    "        history_field_name = cast(str, self._get_history_field_name(signature))\n",
    "        if history_field_name:\n",
    "            # In order to format the conversation history, we need to remove the history field from the signature.\n",
    "            signature_without_history = signature.delete(history_field_name)\n",
    "            conversation_history = self.format_conversation_history(\n",
    "                signature_without_history,\n",
    "                history_field_name,\n",
    "                inputs_copy,\n",
    "            )\n",
    "\n",
    "        _messages = getattr(signature, \"_messages\", Field(default=[])).default\n",
    "\n",
    "        messages = []\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": self._format_template_inputs(\n",
    "                    signature.instructions, inputs_copy\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        messages.extend(self.format_demos(signature, demos))\n",
    "        if history_field_name:\n",
    "            messages.extend(conversation_history)\n",
    "        messages.extend(\n",
    "            [\n",
    "                m | {\"content\": self._format_template_inputs(m[\"content\"], inputs_copy)}\n",
    "                for m in _messages\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        messages = try_expand_image_tags(messages)\n",
    "\n",
    "        return messages\n",
    "\n",
    "    def _format_template_inputs(\n",
    "        self, template: str, inputs: dict[str, Any]\n",
    "    ) -> dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Format the template inputs filling the {{ input }} placeholders.\n",
    "        \"\"\"\n",
    "\n",
    "        class SafeDict(dict):\n",
    "            def __missing__(self, key):\n",
    "                return \"{{\" + key + \"}}\"\n",
    "\n",
    "        template_fmt = template.replace(\"{{\", \"{\").replace(\"}}\", \"}\")\n",
    "        str_inputs: dict[str, str] = {}\n",
    "        for k, v in inputs.items():\n",
    "            str_inputs[k] = (\n",
    "                v\n",
    "                if type(v) == str\n",
    "                else json.dumps(v, cls=SerializableWithStringFallback)\n",
    "            )\n",
    "        return template_fmt.format_map(SafeDict(str_inputs))  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages [{'role': 'system', 'content': 'You are a helpful assistant that answers questions and provides information.'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    importance=<Model.high: 'high'>,\n",
       "    answer='I am here to assist you with any questions or information you need. Please let me know how I can help!'\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dspy\n",
    "from dspy.clients.lm import litellm\n",
    "\n",
    "with langwatch.trace() as trace:\n",
    "    trace.autotrack_dspy()\n",
    "\n",
    "    lm = dspy.LM(model=\"openai/gpt-4o-mini\")\n",
    "    dspy.configure(adapter=TemplateAdapter())\n",
    "\n",
    "\n",
    "    class AnswerImportance(dspy.Signature):\n",
    "        \"\"\"You are a helpful assistant that answers questions and provides information.\"\"\"\n",
    "\n",
    "        _messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"User is asking: {{question}}\"\n",
    "        }]\n",
    "\n",
    "        question: str = dspy.InputField(description=\"The question to answer\")\n",
    "        importance: Model = dspy.OutputField(\n",
    "            description=\"The importance of the answer\"\n",
    "        )\n",
    "        answer: str = dspy.OutputField(description=\"The answer to the question\")\n",
    "\n",
    "    predict = dspy.Predict(AnswerImportance)\n",
    "    predict.set_lm(lm)\n",
    "\n",
    "    prediction = predict(question=\"what is the capital of France???\")\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-BT6f6U4scevsik1gXQJqN7Z9IaQoe', created=1746276820, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_0392822090', choices=[Choices(finish_reason='stop', index=0, message=Message(content='{\"importance\":\"high\",\"answer\":\"The capital of France is Paris.\"}', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]))], usage=Usage(completion_tokens=16, prompt_tokens=292, total_tokens=308, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litellm.completion(\n",
    "    model=\"openai/gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"Your input fields are:\n",
    "1. `question` (str): The question to answer\n",
    "Your output fields are:\n",
    "1. `importance` (Model): The importance of the answer\n",
    "2. `answer` (str): The answer to the question\n",
    "All interactions will be structured in the following way, with the appropriate values filled in.\n",
    "\n",
    "Inputs will have the following structure:\n",
    "\n",
    "[[ ## question ## ]]\n",
    "{question}\n",
    "\n",
    "Outputs will be a JSON object with the following fields.\n",
    "\n",
    "[[ ## importance ## ]]\n",
    "{importance}        # note: the value you produce must adhere to the JSON schema: {\"type\": \"string\", \"title\": \"Model\"}\n",
    "\n",
    "[[ ## answer ## ]]\n",
    "{answer}\n",
    "In adhering to this structure, your objective is:\n",
    "        Given the fields `question`, produce the fields `importance`, `answer`.\"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"[[ ## question ## ]]\n",
    "What is the capital of France????????????????\n",
    "\n",
    "Respond with a JSON object in the following order of fields: `importance` (must be formatted as a valid Python Model), then `answer`.\"\"\",\n",
    "        },\n",
    "        # {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    ],\n",
    "    response_format={\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"DSPyProgramOutputs\",\n",
    "            \"strict\": True,\n",
    "            \"schema\": {\n",
    "                \"additionalProperties\": False,\n",
    "                \"$defs\": {\"Model\": {\"title\": \"Model\", \"type\": \"string\"}},\n",
    "                \"properties\": {\n",
    "                    # \"importance\": {\"title\": \"Importance\", \"type\": \"number\"},\n",
    "                    # \"importance\": {\"$ref\": \"#/$defs/Model\"},\n",
    "                    \"importance\": {\"title\": \"Importance\", \"type\": \"string\", \"enum\": [\"high\", \"medium\", \"low\"]},\n",
    "                    \"answer\": {\"title\": \"Answer\", \"type\": \"string\"},\n",
    "                },\n",
    "                \"required\": [\"importance\", \"answer\"],\n",
    "                \"title\": \"DSPyProgramOutputs\",\n",
    "                \"type\": \"object\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
