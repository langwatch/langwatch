{% macro node_llm_config_to_dspy_lm(llm_config, corrected_params) %}
{% set llm_params = llm_config.litellm_params or {"model": llm_config.model} %}
{% if "azure/" in (llm_params["model"] or "") and "api_version" not in llm_params and "use_azure_gateway" not in llm_params %}
{% set _ = llm_params.update({"api_version": os.environ["AZURE_API_VERSION"]}) %}
{% endif %}
{% set _ = llm_params.update({"drop_params": True, "model_type": "chat"}) %}
dspy.LM(
            max_tokens={{ corrected_params["max_tokens"] }},
            temperature={{ corrected_params["temperature"] }},
            {# Normalize reasoning with correct priority: reasoning > reasoning_effort > thinkingLevel > effort #}
            {# Use 'is not none' to preserve zero values #}
            {% set normalized_reasoning = llm_config.reasoning if llm_config.reasoning is not none else (llm_config.reasoning_effort if llm_config.reasoning_effort is not none else (llm_config.thinkingLevel if llm_config.thinkingLevel is not none else llm_config.effort)) %}
            {# Get effective model from llm_params first, fallback to llm_config.model #}
            {% set model_str = llm_params.get('model') or llm_config.model or '' %}
            {% set provider = model_str.split('/')[0].lower() if '/' in model_str else '' %}
            {# Determine target key based on provider #}
            {% if provider == 'google' or provider == 'gemini' %}
            {% set target_key = 'thinkingLevel' %}
            {% elif provider == 'anthropic' %}
            {% set target_key = 'effort' %}
            {% else %}
            {% set target_key = 'reasoning_effort' %}
            {% endif %}
            {# Emit reasoning param only if normalized_reasoning is not none and not already in llm_params #}
            {% if normalized_reasoning is not none and llm_params.get(target_key) is none %}
            {{ target_key }}={{ normalized_reasoning.__repr__() }},
            {% endif %}
            {# Loop llm_params but skip target_key to avoid duplicates #}
            {% for key, value in llm_params.items() %}
            {% if key != target_key %}
            {{ key }}={{ value.__repr__() }},
            {% endif %}
            {% endfor %}
        )
{% endmacro %}

{% macro edge_param(edge, source_parts, target_parts, use_kwargs) %}
{% if source_parts[0] == "outputs" %}
    {% if edge.source == "entry" %}
{{ target_parts[1] }}={% if use_kwargs %}kwargs.get("{{ source_parts[1] }}"){% else %}{{ source_parts[1] }}{% endif %},
    {% else %}
{{ target_parts[1] }}={{ edge.source }}.{{ source_parts[1] }},
    {% endif %}
{% else %}
    {{ raise("Invalid source: " + source_parts[0] + " for edge " + edge.id) }}
{% endif %}
{% endmacro %}

{% macro edge_param_dict(edge, source_parts, target_parts, use_kwargs) %}
{% if source_parts[0] == "outputs" %}
    {% if edge.source == "entry" %}
"{{ target_parts[1] }}": {% if use_kwargs %}kwargs.get("{{ source_parts[1] }}"){% else %}{{ source_parts[1] }}{% endif %},
    {% else %}
"{{ target_parts[1] }}": {{ edge.source }}.{{ source_parts[1] }},
    {% endif %}
{% else %}
    {{ raise("Invalid source: " + source_parts[0] + " for edge " + edge.id) }}
{% endif %}
{% endmacro %}

{% macro escape_prompt(prompt) %}
""{{ prompt | tojson | replace('\\n', '\n') | replace('\\t', '\t') | replace('\\"', '"') | replace('"""', '\\"\\"\\"') }}""{% endmacro %}
