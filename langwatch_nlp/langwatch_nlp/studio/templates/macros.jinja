{% macro node_llm_config_to_dspy_lm(llm_config, corrected_params) %}
{% set llm_params = llm_config.litellm_params or {"model": llm_config.model} %}
{% if "azure/" in (llm_params["model"] or "") and "api_version" not in llm_params and "use_azure_gateway" not in llm_params %}
{% set _ = llm_params.update({"api_version": os.environ["AZURE_API_VERSION"]}) %}
{% endif %}
{% set _ = llm_params.update({"drop_params": True, "model_type": "chat"}) %}
dspy.LM(
            max_tokens={{ corrected_params["max_tokens"] }},
            temperature={{ corrected_params["temperature"] }},
            {% for key, value in llm_params.items() %}
            {{ key }}={{ value.__repr__() }},
            {% endfor %}
        )
{% endmacro %}

{% macro edge_param(edge, source_parts, target_parts, use_kwargs) %}
{% if source_parts[0] == "outputs" %}
    {% if edge.source == "entry" %}
{{ target_parts[1] }}={% if use_kwargs %}kwargs.get("{{ source_parts[1] }}"){% else %}{{ source_parts[1] }}{% endif %},
    {% else %}
{{ target_parts[1] }}={{ edge.source }}.{{ source_parts[1] }},
    {% endif %}
{% else %}
    {{ raise("Invalid source: " + source_parts[0] + " for edge " + edge.id) }}
{% endif %}
{% endmacro %}

{% macro edge_param_dict(edge, source_parts, target_parts, use_kwargs) %}
{% if source_parts[0] == "outputs" %}
    {% if edge.source == "entry" %}
"{{ target_parts[1] }}": {% if use_kwargs %}kwargs.get("{{ source_parts[1] }}"){% else %}{{ source_parts[1] }}{% endif %},
    {% else %}
"{{ target_parts[1] }}": {{ edge.source }}.{{ source_parts[1] }},
    {% endif %}
{% else %}
    {{ raise("Invalid source: " + source_parts[0] + " for edge " + edge.id) }}
{% endif %}
{% endmacro %}

{% macro escape_prompt(prompt) %}
""{{ prompt | tojson | replace('\\n', '\n') | replace('\\t', '\t') | replace('\\"', '"') | replace('"""', '\\"\\"\\"') }}""{% endmacro %}
