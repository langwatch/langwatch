{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cde8a9dc",
      "metadata": {},
      "source": [
        "# LangWatch Evaluation Tracking\n",
        "\n",
        "## Simple Evaluation Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4b3c5de",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangWatch API key is already set, if you want to login again, please call as langwatch.login(relogin=True)\n"
          ]
        }
      ],
      "source": [
        "import langwatch\n",
        "\n",
        "langwatch.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6f664279",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Follow the results at: http://localhost:5560/inbox-narrator/experiments/my-incredible-experiment?runId=eggplant-trout-of-painting\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 7/7 [00:42<00:00,  6.05s/it]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    [\n",
        "        {\n",
        "            \"question\": \"What is LangWatch?\",\n",
        "            \"answer\": \"LangWatch is a platform for evaluating and improving language models.\",\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"How do I use LangWatch?\",\n",
        "            \"answer\": \"You can use LangWatch by installing the LangWatch SDK and then calling the LangWatch API.\",\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"Does LangWatch support multiple language models?\",\n",
        "            \"answer\": \"Yes, LangWatch is compatible with all language models by using LiteLLM under the hood.\",\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"Can I visualize evaluation metrics in LangWatch?\",\n",
        "            \"answer\": \"Yes, LangWatch provides dashboards for visualizing key evaluation metrics.\",\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"Is there a free tier for LangWatch?\",\n",
        "            \"answer\": \"LangWatch offers a free tier with limited usage, ideal for small projects and evaluation.\",\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"Where can I find documentation for LangWatch?\",\n",
        "            \"answer\": \"You can find the official documentation on the LangWatch website or GitHub repository.\",\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"![](https://i.imgur.com/Tb5hyby.jpeg)\",\n",
        "            \"answer\": \"This is a screenshot of LangWatch website\"\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "evaluation = langwatch.experiment.init(\"my-incredible-experiment\")\n",
        "\n",
        "\n",
        "@langwatch.trace()\n",
        "def agent(question):\n",
        "    time.sleep(random.randint(0, 10))\n",
        "    return {\"text\": \"foo bar\"}\n",
        "\n",
        "\n",
        "for index, row in evaluation.loop(df.iterrows()):\n",
        "    result = agent(row[\"question\"])  # your code\n",
        "\n",
        "    score = random.randint(0, 80) / 100 + 0.2\n",
        "    evaluation.log(\"sample_metric\", index=index, score=score, passed=score > 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bf2e191",
      "metadata": {},
      "source": [
        "## Parallel Evaluation Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fd7b230",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-01-15 12:44:09,318 - langwatch.client - WARNING - An existing global tracer provider was found. Attaching LangWatch exporter to the existing provider. Set `ignore_global_tracer_provider_override_warning=True` to suppress this warning.\n",
            "2026-01-15 12:44:09,320 - langwatch.client - INFO - Configuring OTLP exporter with endpoint: http://localhost:5560/api/otel/v1/traces\n",
            "Follow the results at: http://localhost:5560/inbox-narrator/experiments/my-incredible-experiment?runId=prophetic-uakari-of-satiation\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 7/7 [00:13<00:00,  1.91s/it]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "langwatch.setup()\n",
        "evaluation = langwatch.experiment.init(\"my-incredible-experiment\")\n",
        "\n",
        "@langwatch.trace()\n",
        "def agent(question):\n",
        "    time.sleep(random.randint(0, 10))\n",
        "    return \"foo parallel\"\n",
        "\n",
        "for index, row in evaluation.loop(df.iterrows(), threads=4):\n",
        "    def task(index, row):\n",
        "        result = agent(row[\"question\"])\n",
        "        evaluation.log(\"sample_metric\", index=index, data={\"response\": result}, score=1)\n",
        "    evaluation.submit(task, index, row)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f3d8be1",
      "metadata": {},
      "source": [
        "## Multi-Target Comparison\n",
        "\n",
        "Use `evaluation.target()` to compare multiple models/configurations on the same dataset.\n",
        "Each target gets its own dataset entry with automatic latency tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "84f38e09",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-01-15 12:44:34,995 - langwatch.client - WARNING - An existing global tracer provider was found. Attaching LangWatch exporter to the existing provider. Set `ignore_global_tracer_provider_override_warning=True` to suppress this warning.\n",
            "2026-01-15 12:44:34,997 - langwatch.client - INFO - Configuring OTLP exporter with endpoint: http://localhost:5560/api/otel/v1/traces\n",
            "Follow the results at: http://localhost:5560/inbox-narrator/experiments/model-comparison-experiment?runId=versatile-arrogant-woodpecker\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.38it/s]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "langwatch.setup()\n",
        "evaluation = langwatch.experiment.init(\"model-comparison-experiment\")\n",
        "\n",
        "\n",
        "@langwatch.trace()\n",
        "def call_model(model_name: str, question: str):\n",
        "    \"\"\"Simulate calling different LLM models.\"\"\"\n",
        "    # Simulate different latencies per model\n",
        "    latency = {\"gpt-4\": 0.5, \"gpt-3.5\": 0.2, \"claude\": 0.3}.get(model_name, 0.1)\n",
        "    time.sleep(latency + random.random() * 0.2)\n",
        "    return f\"Response from {model_name}: This is a simulated answer to '{question[:30]}...'\"\n",
        "\n",
        "\n",
        "for index, row in evaluation.loop(df.iterrows(), threads=4):\n",
        "    def task(index, row):\n",
        "        # Compare GPT-4 vs GPT-3.5 vs Claude on the same question\n",
        "        with evaluation.target(\"gpt-4\", {\"model\": \"openai/gpt-4\", \"temperature\": 0.7}):\n",
        "            response = call_model(\"gpt-4\", row[\"question\"])\n",
        "            evaluation.log_response(response)  # Store the model output\n",
        "            score = random.uniform(0.8, 1.0)  # GPT-4 tends to score higher\n",
        "            evaluation.log(\"quality\", index=index, score=score)\n",
        "\n",
        "        with evaluation.target(\"gpt-3.5\", {\"model\": \"openai/gpt-3.5-turbo\", \"temperature\": 0.7}):\n",
        "            response = call_model(\"gpt-3.5\", row[\"question\"])\n",
        "            evaluation.log_response(response)\n",
        "            score = random.uniform(0.6, 0.9)  # GPT-3.5 scores a bit lower\n",
        "            evaluation.log(\"quality\", index=index, score=score)\n",
        "\n",
        "        with evaluation.target(\"claude\", {\"model\": \"anthropic/claude-3-sonnet\", \"temperature\": 0.7}):\n",
        "            response = call_model(\"claude\", row[\"question\"])\n",
        "            evaluation.log_response(response)\n",
        "            score = random.uniform(0.75, 0.95)  # Claude in between\n",
        "            evaluation.log(\"quality\", index=index, score=score)\n",
        "\n",
        "    evaluation.submit(task, index, row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9fa8b21",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
