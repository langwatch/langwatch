{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde8a9dc",
   "metadata": {},
   "source": [
    "# LangWatch Evaluation Tracking\n",
    "\n",
    "## Simple Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f664279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Follow the results at: http://localhost:5560/inbox-narrator/experiments/my-incredible-experiment?runId=industrious-convivial-waxbill\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14788d032df4066b721bce26ba5eb48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import langwatch\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"question\": \"What is LangWatch?\",\n",
    "            \"answer\": \"LangWatch is a platform for evaluating and improving language models.\",\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How do I use LangWatch?\",\n",
    "            \"answer\": \"You can use LangWatch by installing the LangWatch SDK and then calling the LangWatch API.\",\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Does LangWatch support multiple language models?\",\n",
    "            \"answer\": \"Yes, LangWatch is compatible with all language models by using LiteLLM under the hood.\",\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Can I visualize evaluation metrics in LangWatch?\",\n",
    "            \"answer\": \"Yes, LangWatch provides dashboards for visualizing key evaluation metrics.\",\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Is there a free tier for LangWatch?\",\n",
    "            \"answer\": \"LangWatch offers a free tier with limited usage, ideal for small projects and evaluation.\",\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Where can I find documentation for LangWatch?\",\n",
    "            \"answer\": \"You can find the official documentation on the LangWatch website or GitHub repository.\",\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "evaluation = langwatch.evaluation.init(\"my-incredible-experiment\")\n",
    "\n",
    "\n",
    "@langwatch.trace()\n",
    "def agent(question):\n",
    "    time.sleep(random.randint(0, 10) / 10)\n",
    "    return {\"text\": \"foo bar\"}\n",
    "\n",
    "\n",
    "for index, row in evaluation.loop(df.iterrows()):\n",
    "    result = agent(row[\"question\"])  # your code\n",
    "\n",
    "    score = random.randint(0, 80) / 100 + 0.2\n",
    "    evaluation.log(\"sample_metric\", index=index, score=score, passed=score > 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf2e191",
   "metadata": {},
   "source": [
    "## Parallel Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fd7b230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-24 09:17:59,631 - langwatch.client - INFO - Registering atexit handler to flush tracer provider on exit\n",
      "2025-05-24 09:17:59,632 - langwatch.client - WARNING - An existing global trace provider was found. LangWatch will not override it automatically, but instead is attaching another span processor and exporter to it. You can disable this warning by setting `ignore_global_tracer_provider_override_warning` to `True`.\n",
      "Follow the results at: http://localhost:5560/inbox-narrator/experiments/my-incredible-experiment?runId=flawless-conscious-lori\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff04a6a81d8410394828e1276aa3d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detach context\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rchaves/Projects/langwatch-saas/langwatch/python-sdk/.venv/lib/python3.11/site-packages/pydantic/type_adapter.py\", line 271, in _init_core_attrs\n",
      "    self.core_schema = _getattr_no_parents(self._type, '__pydantic_core_schema__')\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rchaves/Projects/langwatch-saas/langwatch/python-sdk/.venv/lib/python3.11/site-packages/pydantic/type_adapter.py\", line 55, in _getattr_no_parents\n",
      "    raise AttributeError(attribute)\n",
      "AttributeError: __pydantic_core_schema__\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rchaves/Projects/langwatch-saas/langwatch/python-sdk/.venv/lib/python3.11/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n",
      "    _RUNTIME_CONTEXT.detach(token)\n",
      "  File \"/Users/rchaves/Projects/langwatch-saas/langwatch/python-sdk/.venv/lib/python3.11/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n",
      "    self._current_context.reset(token)\n",
      "ValueError: <Token var=<ContextVar name='current_context' default={} at 0x10fa28130> at 0x3270b85c0> was created in a different Context\n",
      "Failed to detach context\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rchaves/Projects/langwatch-saas/langwatch/python-sdk/.venv/lib/python3.11/site-packages/pydantic/type_adapter.py\", line 271, in _init_core_attrs\n",
      "    self.core_schema = _getattr_no_parents(self._type, '__pydantic_core_schema__')\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rchaves/Projects/langwatch-saas/langwatch/python-sdk/.venv/lib/python3.11/site-packages/pydantic/type_adapter.py\", line 55, in _getattr_no_parents\n",
      "    raise AttributeError(attribute)\n",
      "AttributeError: __pydantic_core_schema__\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rchaves/Projects/langwatch-saas/langwatch/python-sdk/.venv/lib/python3.11/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n",
      "    _RUNTIME_CONTEXT.detach(token)\n",
      "  File \"/Users/rchaves/Projects/langwatch-saas/langwatch/python-sdk/.venv/lib/python3.11/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n",
      "    self._current_context.reset(token)\n",
      "ValueError: <Token var=<ContextVar name='current_context' default={} at 0x10fa28130> at 0x3270b9a00> was created in a different Context\n",
      "Failed to detach context\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rchaves/Projects/langwatch-saas/langwatch/python-sdk/.venv/lib/python3.11/site-packages/pydantic/type_adapter.py\", line 271, in _init_core_attrs\n",
      "    self.core_schema = _getattr_no_parents(self._type, '__pydantic_core_schema__')\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rchaves/Projects/langwatch-saas/langwatch/python-sdk/.venv/lib/python3.11/site-packages/pydantic/type_adapter.py\", line 55, in _getattr_no_parents\n",
      "    raise AttributeError(attribute)\n",
      "AttributeError: __pydantic_core_schema__\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rchaves/Projects/langwatch-saas/langwatch/python-sdk/.venv/lib/python3.11/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n",
      "    _RUNTIME_CONTEXT.detach(token)\n",
      "  File \"/Users/rchaves/Projects/langwatch-saas/langwatch/python-sdk/.venv/lib/python3.11/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n",
      "    self._current_context.reset(token)\n",
      "ValueError: <Token var=<ContextVar name='current_context' default={} at 0x10fa28130> at 0x32700c680> was created in a different Context\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "langwatch.setup()\n",
    "evaluation = langwatch.evaluation.init(\"my-incredible-experiment\")\n",
    "\n",
    "@langwatch.trace()\n",
    "def agent(question):\n",
    "    time.sleep(random.randint(0, 10) / 10)\n",
    "    return \"foo parallel\"\n",
    "\n",
    "for index, row in evaluation.loop(df.iterrows(), threads=4):\n",
    "    def evaluate(index, row):\n",
    "        result = agent(row[\"question\"])\n",
    "        evaluation.log(\"sample_metric\", index=index, data={\"response\": result}, score=1)\n",
    "    evaluation.submit(evaluate, index, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8157c8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_is_copy': None, '_mgr': SingleBlockManager\n",
      "Items: Index(['question', 'answer'], dtype='object')\n",
      "NumpyBlock: 2 dtype: object, '_item_cache': {}, '_attrs': {}, '_flags': <Flags(allows_duplicate_labels=True)>, '_name': 0}\n",
      "{'_is_copy': None, '_mgr': SingleBlockManager\n",
      "Items: Index(['question', 'answer'], dtype='object')\n",
      "NumpyBlock: 2 dtype: object, '_item_cache': {}, '_attrs': {}, '_flags': <Flags(allows_duplicate_labels=True)>, '_name': 1}\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    print(row.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a61c2b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "retrieved_documents: ['Introduction - LangWatchLangWatch home pageSearch...llms.txtSupportDashboardlangwatch/langwatchlangwatch/langwatchSearch...NavigationGet StartedIntroductionDocumentationOpen DashboardGitHub RepoGet StartedIntroductionSelf HostingCookbooksLLM ObservabilityOverviewConceptsLanguage APIs & SDKsUser EventsMonitoring & AlertsCode ExamplesLLM EvaluationOffline EvaluationReal-Time EvaluationList of EvaluatorsDatasetsAnnotationsLLM DevelopmentPrompt Optimization StudioDSPy VisualizationLangWatch MCPPrompt VersioningAPI EndpointsTracesPromptsAnnotationsDatasetsSupportTroubleshooting and SupportStatus PageGet StartedIntroductionCopy pageWelcome to LangWatch, the all-in-one open-source LLMops platform.LangWatch allows you to track, monitor, guardrail and evaluate your LLMs apps for measuring quality and alert on issues.\\nFor domain experts, it allows you to easily sift through conversations, see topics being discussed and annotate and score messages', 'For domain experts, it allows you to easily sift through conversations, see topics being discussed and annotate and score messages\\nfor improvement in a collaborative manner with the development team.\\nFor developers, it allows you to debug, build datasets, prompt engineer on the playground and\\nrun batch evaluations or DSPy experiments to continuously improve the product.\\nFinally, for the business, it allows you to track conversation metrics and give full user and quality analytics, cost tracking, build\\ncustom dashboards and even integrate it back on your own platform for reporting to your customers.\\nYou can sign up and already start the integration on our free tier by following the guides bellow:\\nPython Integration GuideTypeScript Integration GuideREST API\\nYou can also open the demo project check out a video on our platform.\\n\\u200bGet in touch\\nFeel free to reach out to us directly at [email\\xa0protected]. You can also open a GitHub issue', 'You can also open the demo project check out a video on our platform.\\n\\u200bGet in touch\\nFeel free to reach out to us directly at [email\\xa0protected]. You can also open a GitHub issue\\nto report bugs and request features, or join our Discord channel and ask questions directly for the community and the core team.Was this page helpful?YesNoOverviewgithubwebsitex-twitterlinkedinyoutubePowered by MintlifyOn this pageGet in touch']\n",
      "output: LangWatch is an open-source LLMops platform for tracking, monitoring, and evaluating LLM applications, aimed at improving quality and user experience. 🌐\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"langwatch/python-sdk/.env\")\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_core.vectorstores.base import VectorStoreRetriever\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.langwatch.ai\")\n",
    "docs = loader.load()\n",
    "documents = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ").split_documents(docs)\n",
    "\n",
    "vector = FAISS.from_documents(documents, OpenAIEmbeddings())\n",
    "retriever = vector.as_retriever()\n",
    "\n",
    "retrieved_documents = []\n",
    "\n",
    "# Wrap the FAISS retriever so that we can capture which documents were used to generate the response\n",
    "@tool\n",
    "def langwatch_search(\n",
    "    query: str\n",
    ") -> list[Document]:\n",
    "    \"\"\"\"Search for information about LangWatch. For any questions about LangWatch, use this tool if you didn't already\"\"\"\n",
    "\n",
    "    global retrieved_documents\n",
    "    retrieved_documents = retriever.get_relevant_documents(query)\n",
    "    return retrieved_documents\n",
    "\n",
    "tools = [langwatch_search]\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that only reply in short tweet-like responses, use tools only once.\\n\\n{agent_scratchpad}\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "agent = create_tool_calling_agent(model, tools, prompt)\n",
    "executor = AgentExecutor(agent=agent, tools=tools, verbose=False)  # type: ignore\n",
    "\n",
    "output = executor.invoke({\"question\": \"What is LangWatch?\"})[\"output\"]\n",
    "\n",
    "print(\"\")\n",
    "print(\"retrieved_documents:\", [ d.page_content for d in retrieved_documents])\n",
    "print(\"output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac02587",
   "metadata": {},
   "source": [
    "## Step 2: Run the Offline Evaluation\n",
    "\n",
    "Now we can use the dataset we have from LangWatch to run a batch evaluation experiment through our LLM pipeline, to see the results and tweak it for optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47459a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Follow the results at: http://localhost:5560/inbox-narrator/experiments/my-incredible-experiment?runId=aloof-simple-mammoth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52acbd5759f64823aa931ee7f7eb55b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'executor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m evaluation = langwatch.evaluation.init(\u001b[33m\"\u001b[39m\u001b[33mmy-incredible-experiment\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m evaluation.loop(df.iterrows()):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     response = \u001b[43mexecutor\u001b[49m.invoke({\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: row[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m]})[\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     42\u001b[39m     evaluation.log(\u001b[33m\"\u001b[39m\u001b[33mmrr\u001b[39m\u001b[33m\"\u001b[39m, index=index, data={\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m: response}, score=\u001b[32m0.5\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'executor' is not defined"
     ]
    }
   ],
   "source": [
    "import langwatch\n",
    "import pandas as pd\n",
    "\n",
    "# Create a dataset\n",
    "df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"question\": \"What is LangWatch?\",\n",
    "            \"answer\": \"LangWatch is a platform for evaluating and improving language models.\",\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How do I use LangWatch?\",\n",
    "            \"answer\": \"You can use LangWatch by installing the LangWatch SDK and then calling the LangWatch API.\",\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Does LangWatch support multiple language models?\",\n",
    "            \"answer\": \"Yes, LangWatch is compatible with all language models by using LiteLLM under the hood.\",\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Can I visualize evaluation metrics in LangWatch?\",\n",
    "            \"answer\": \"Yes, LangWatch provides dashboards for visualizing key evaluation metrics.\",\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Is there a free tier for LangWatch?\",\n",
    "            \"answer\": \"LangWatch offers a free tier with limited usage, ideal for small projects and evaluation.\",\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Where can I find documentation for LangWatch?\",\n",
    "            \"answer\": \"You can find the official documentation on the LangWatch website or GitHub repository.\",\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "# Or retrieve it from LangWatch:\n",
    "# df = langwatch.dataset.get_dataset(\"CEtFivQeya4kyAzy9eJht\").to_pandas()  # dataset--rSAYL4HxQRXHSayq6c7A\n",
    "\n",
    "\n",
    "evaluation = langwatch.evaluation.init(\"my-incredible-experiment\")\n",
    "\n",
    "for index, row in evaluation.loop(df.iterrows()):\n",
    "    response = executor.invoke({\"question\": row[\"question\"]})[\"output\"]\n",
    "\n",
    "    evaluation.run(\n",
    "        \"ragas/faithfulness\",\n",
    "        index=index,\n",
    "        data={\n",
    "            \"input\": row[\"question\"],\n",
    "            \"output\": response,\n",
    "            \"contexts\": [d.page_content for d in retrieved_documents],\n",
    "        },\n",
    "        settings={\n",
    "            \"model\": \"openai/gpt-4o-mini\",\n",
    "            \"max_tokens\": 2048,\n",
    "            \"autodetect_dont_know\": True,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4041d-b469-42d9-affc-f7ccbc03c98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch evaluation...\n",
      "Follow the results at: https://app.langwatch.ai/public-documentation-examples-YxS3Bf/experiments/langwatch-rag-experiment?runId=mysterious-peculiar-cricket\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:45<00:00,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch evaluation done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>contexts</th>\n",
       "      <th>jailbreak-detection</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can I customize the evaluation metrics in Lang...</td>\n",
       "      <td>Can I customize the evaluation metrics in Lang...</td>\n",
       "      <td>Yes, you can customize evaluation metrics in L...</td>\n",
       "      <td>[Introduction - LangWatchLangWatch home pageSe...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What programming languages are supported by La...</td>\n",
       "      <td>What programming languages are supported by La...</td>\n",
       "      <td>LangWatch supports Python and TypeScript for i...</td>\n",
       "      <td>[Introduction - LangWatchLangWatch home pageSe...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do I configure alerts in LangWatch?</td>\n",
       "      <td>How do I configure alerts in LangWatch?</td>\n",
       "      <td>Check the LangWatch documentation for configur...</td>\n",
       "      <td>[Introduction - LangWatchLangWatch home pageSe...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Does it support langchain?</td>\n",
       "      <td>Does it support langchain?</td>\n",
       "      <td>Yes, LangWatch supports LangChain! You can tra...</td>\n",
       "      <td>[Introduction - LangWatchLangWatch home pageSe...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How does LangWatch help in LLMOps?</td>\n",
       "      <td>How does LangWatch help in LLMOps?</td>\n",
       "      <td>LangWatch aids in LLMOps by tracking, monitori...</td>\n",
       "      <td>[Introduction - LangWatchLangWatch home pageSe...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How can I visualize the traces collected by La...</td>\n",
       "      <td>How can I visualize the traces collected by La...</td>\n",
       "      <td>Check out the LangWatch documentation for guid...</td>\n",
       "      <td>[Introduction - LangWatchLangWatch home pageSe...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Is it possible to automate evaluations with La...</td>\n",
       "      <td>Is it possible to automate evaluations with La...</td>\n",
       "      <td>Yes, LangWatch allows for batch evaluations an...</td>\n",
       "      <td>[Introduction - LangWatchLangWatch home pageSe...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What programming languages are supported by La...</td>\n",
       "      <td>What programming languages are supported by La...</td>\n",
       "      <td>Agent stopped due to max iterations.</td>\n",
       "      <td>[Introduction - LangWatchLangWatch home pageSe...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How do I set up LangWatch in my python app?</td>\n",
       "      <td>How do I set up LangWatch in my python app?</td>\n",
       "      <td>Check out the [Python Integration Guide](https...</td>\n",
       "      <td>[Introduction - LangWatchLangWatch home pageSe...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are the best practices for using LangWatc...</td>\n",
       "      <td>What are the best practices for using LangWatc...</td>\n",
       "      <td>1. **Integration**: Follow Python, TypeScript,...</td>\n",
       "      <td>[Introduction - LangWatchLangWatch home pageSe...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Can I customize the evaluation metrics in Lang...   \n",
       "1  What programming languages are supported by La...   \n",
       "2            How do I configure alerts in LangWatch?   \n",
       "3                         Does it support langchain?   \n",
       "4                 How does LangWatch help in LLMOps?   \n",
       "5  How can I visualize the traces collected by La...   \n",
       "6  Is it possible to automate evaluations with La...   \n",
       "7  What programming languages are supported by La...   \n",
       "8        How do I set up LangWatch in my python app?   \n",
       "9  What are the best practices for using LangWatc...   \n",
       "\n",
       "                                               input  \\\n",
       "0  Can I customize the evaluation metrics in Lang...   \n",
       "1  What programming languages are supported by La...   \n",
       "2            How do I configure alerts in LangWatch?   \n",
       "3                         Does it support langchain?   \n",
       "4                 How does LangWatch help in LLMOps?   \n",
       "5  How can I visualize the traces collected by La...   \n",
       "6  Is it possible to automate evaluations with La...   \n",
       "7  What programming languages are supported by La...   \n",
       "8        How do I set up LangWatch in my python app?   \n",
       "9  What are the best practices for using LangWatc...   \n",
       "\n",
       "                                              output  \\\n",
       "0  Yes, you can customize evaluation metrics in L...   \n",
       "1  LangWatch supports Python and TypeScript for i...   \n",
       "2  Check the LangWatch documentation for configur...   \n",
       "3  Yes, LangWatch supports LangChain! You can tra...   \n",
       "4  LangWatch aids in LLMOps by tracking, monitori...   \n",
       "5  Check out the LangWatch documentation for guid...   \n",
       "6  Yes, LangWatch allows for batch evaluations an...   \n",
       "7               Agent stopped due to max iterations.   \n",
       "8  Check out the [Python Integration Guide](https...   \n",
       "9  1. **Integration**: Follow Python, TypeScript,...   \n",
       "\n",
       "                                            contexts  jailbreak-detection  \\\n",
       "0  [Introduction - LangWatchLangWatch home pageSe...                 True   \n",
       "1  [Introduction - LangWatchLangWatch home pageSe...                 True   \n",
       "2  [Introduction - LangWatchLangWatch home pageSe...                 True   \n",
       "3  [Introduction - LangWatchLangWatch home pageSe...                 True   \n",
       "4  [Introduction - LangWatchLangWatch home pageSe...                 True   \n",
       "5  [Introduction - LangWatchLangWatch home pageSe...                 True   \n",
       "6  [Introduction - LangWatchLangWatch home pageSe...                 True   \n",
       "7  [Introduction - LangWatchLangWatch home pageSe...                 True   \n",
       "8  [Introduction - LangWatchLangWatch home pageSe...                 True   \n",
       "9  [Introduction - LangWatchLangWatch home pageSe...                 True   \n",
       "\n",
       "   faithfulness  \n",
       "0      0.000000  \n",
       "1      1.000000  \n",
       "2      0.000000  \n",
       "3      0.666667  \n",
       "4      0.888889  \n",
       "5      0.000000  \n",
       "6      0.666667  \n",
       "7      0.000000  \n",
       "8      0.000000  \n",
       "9      1.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langwatch.batch_evaluation import BatchEvaluation, DatasetEntry\n",
    "\n",
    "\n",
    "def callback(entry: DatasetEntry):\n",
    "    output = executor.invoke({\"question\": entry[\"question\"]})[\"output\"]\n",
    "\n",
    "    return {\"input\": entry[\"question\"], \"output\": output, \"contexts\": [d.page_content for d in retrieved_documents]}\n",
    "\n",
    "# Instantiate the BatchEvaluation object\n",
    "evaluation = BatchEvaluation(\n",
    "    experiment=\"LangWatch RAG Experiment\",\n",
    "    dataset=\"dataset--rSAYL4HxQRXHSayq6c7A\",\n",
    "    evaluations=[\"jailbreak-detection\", \"faithfulness\"],\n",
    "    callback=callback,\n",
    ")\n",
    "\n",
    "# Run the evaluation\n",
    "results = evaluation.run()\n",
    "results.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fa8b21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
